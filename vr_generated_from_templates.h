//Generated by './generateBackendInterOperator.py'
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verroucast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation cast backend mcaquad


static VG_REGPARM(3) Int vr_mcaquadcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_mcaquad_cast_double_to_float(*arg1, &res,backend_mcaquad_context);
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation cast backend checkdenorm


static VG_REGPARM(3) Int vr_checkdenormcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_checkdenorm_cast_double_to_float(*arg1, &res,backend_checkdenorm_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation cast backend mcaquad


static VG_REGPARM(3) Int vr_mcaquad_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_cast_double_to_float(*arg1, &res,backend_mcaquad_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation cast backend checkdenorm


static VG_REGPARM(3) Int vr_checkdenorm_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_cast_double_to_float(*arg1, &res,backend_checkdenorm_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verroucheck_float_maxcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res,backend_verrou_context);
  interflop_check_float_max_cast_double_to_float(*arg1, &res,backend_check_float_max_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verroucheck_float_max_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float(*arg1, &res,backend_verrou_context);
  interflop_check_float_max_cast_double_to_float(*arg1, &res,backend_check_float_max_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_NEARESTcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_UPWARDcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_UPWARD(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_DOWNWARDcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_DOWNWARD(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_FARTHESTcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_FARTHEST(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_ZEROcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_ZERO(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AWAY_ZEROcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_AWAY_ZERO(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOMcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_RANDOM(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_DETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_RANDOM_DET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_RANDOM_COMDET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGEcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_AVERAGE(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_DETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_AVERAGE_DET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_AVERAGE_COMDET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOMcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_PRANDOM(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOM_DETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_PRANDOM_DET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_PRANDOM_COMDET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_RANDOM_SCOMDET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDETcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_AVERAGE_SCOMDET(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_SR_MONOTONICcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_SR_MONOTONIC(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONICcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float_SR_SMONOTONIC(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_UPWARD_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_UPWARD(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_DOWNWARD_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_DOWNWARD(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_FARTHEST_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_FARTHEST(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_ZERO_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_ZERO(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AWAY_ZERO_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_AWAY_ZERO(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_RANDOM(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_DET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_RANDOM_DET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_RANDOM_COMDET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_AVERAGE(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_DET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_AVERAGE_DET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_AVERAGE_COMDET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOM_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_PRANDOM(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOM_DET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_PRANDOM_DET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_PRANDOM_COMDET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_RANDOM_SCOMDET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDET_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_AVERAGE_SCOMDET(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_SR_MONOTONIC_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_SR_MONOTONIC(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend verrou


static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONIC_softcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_cast_double_to_float_SR_SMONOTONIC(*arg1, &res,backend_verrou_context);
}else{
  interflop_verrou_cast_double_to_float_NEAREST(*arg1, &res,backend_verrou_null_context);
}
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_SQRT
// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrousqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrousqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrousqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrousqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_checkdenorm_sqrt_double(*arg1, &res, backend_checkdenorm_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_sqrt_double(arg1[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_sqrt_double(arg1[1], res+1, backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sqrt_double(arg1[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_checkdenorm_sqrt_float(*arg1, &res, backend_checkdenorm_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sqrt_float(arg1[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sqrt_float(arg1[i], res+i, backend_checkdenorm_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenorm_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_sqrt_double(*arg1, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_sqrt_double(arg1[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_sqrt_double(arg1[1], res+1, backend_checkdenorm_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenorm_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sqrt_double(arg1[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenorm_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_sqrt_float(*arg1, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sqrt_float(arg1[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenorm_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sqrt_float(arg1[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_maxsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double(*arg1, &res, backend_verrou_context);
  interflop_check_float_max_sqrt_double(*arg1, &res, backend_check_float_max_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double(arg1[0], res, backend_verrou_context);
  interflop_check_float_max_sqrt_double(arg1[0], res, backend_check_float_max_context);
  interflop_verrou_sqrt_double(arg1[1], res+1, backend_verrou_context);
  interflop_check_float_max_sqrt_double(arg1[1], res+1, backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_double(arg1[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float(*arg1, &res, backend_verrou_context);
  interflop_check_float_max_sqrt_float(*arg1, &res, backend_check_float_max_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_float(arg1[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_float(arg1[i], res+i, backend_check_float_max_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_max_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double(*arg1, &res, backend_verrou_context);
  interflop_check_float_max_sqrt_double(*arg1, &res, backend_check_float_max_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double(arg1[0], res, backend_verrou_context);
  interflop_check_float_max_sqrt_double(arg1[0], res, backend_check_float_max_context);
  interflop_verrou_sqrt_double(arg1[1], res+1, backend_verrou_context);
  interflop_check_float_max_sqrt_double(arg1[1], res+1, backend_check_float_max_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_double(arg1[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheck_float_max_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float(*arg1, &res, backend_verrou_context);
  interflop_check_float_max_sqrt_float(*arg1, &res, backend_check_float_max_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_float(arg1[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float(arg1[i], res+i, backend_verrou_context);
     interflop_check_float_max_sqrt_float(arg1[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_NEARESTsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_NEARESTsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_NEARESTsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_NEARESTsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARDsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_UPWARD(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_UPWARD(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_UPWARD(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_UPWARDsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_UPWARDsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_UPWARD(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_UPWARDsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARDsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_DOWNWARD(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_DOWNWARD(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_DOWNWARD(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARDsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_DOWNWARD(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHESTsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_FARTHEST(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_FARTHEST(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_FARTHEST(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_FARTHESTsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_FARTHEST(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_ZEROsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_ZERO(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_ZERO(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_ZERO(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_ZEROsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_ZEROsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_ZERO(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_ZEROsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZEROsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_AWAY_ZERO(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_AWAY_ZERO(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AWAY_ZERO(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZEROsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_AWAY_ZERO(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOMsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_RANDOM(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_RANDOM(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOMsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOMsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_RANDOM(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOMsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_RANDOM_DET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_RANDOM_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_DET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_RANDOM_DET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_RANDOM_COMDET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_RANDOM_COMDET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGEsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_AVERAGE(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_AVERAGE(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGEsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_AVERAGE(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_AVERAGE_DET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_AVERAGE_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_DET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_AVERAGE_DET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_AVERAGE_COMDET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_AVERAGE_COMDET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOMsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_PRANDOM(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_PRANDOM(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOMsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_PRANDOM(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_PRANDOM_DET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_PRANDOM_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM_DET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_PRANDOM_DET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_PRANDOM_COMDET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_PRANDOM_COMDET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_RANDOM_SCOMDET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDETsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDETsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_AVERAGE_SCOMDET(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONICsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_SR_MONOTONIC(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONICsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_SR_MONOTONIC(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONICsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
  interflop_verrou_sqrt_double_SR_SMONOTONIC(*arg1, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
  interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONICsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
  interflop_verrou_sqrt_float_SR_SMONOTONIC(*arg1, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARD_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_UPWARD(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_UPWARD(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_UPWARD(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_UPWARD_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_UPWARD(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_UPWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARD_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_DOWNWARD(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_DOWNWARD(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_DOWNWARD(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARD_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_DOWNWARD(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_DOWNWARD(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHEST_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_FARTHEST(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_FARTHEST(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_FARTHEST(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_FARTHEST_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_FARTHEST(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_FARTHEST(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_ZERO_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_ZERO(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_ZERO(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_ZERO(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_ZERO_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_ZERO(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZERO_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AWAY_ZERO(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AWAY_ZERO(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AWAY_ZERO(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZERO_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_AWAY_ZERO(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AWAY_ZERO(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_RANDOM(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_DET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_RANDOM_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_RANDOM_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_AVERAGE(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_DET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_AVERAGE_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_AVERAGE_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_PRANDOM(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM_DET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM_DET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_PRANDOM_DET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM_DET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_PRANDOM_COMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_PRANDOM_COMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_RANDOM_SCOMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_RANDOM_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDET_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDET_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_AVERAGE_SCOMDET(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_AVERAGE_SCOMDET(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONIC_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_SR_MONOTONIC(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONIC_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_SR_MONOTONIC(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_SR_MONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sqrt backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONIC_softsqrt64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_SR_SMONOTONIC(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsqrt64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[0], res, backend_verrou_context);
  interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sqrt_double_NEAREST(arg1[0], res, backend_verrou_null_context);
  interflop_verrou_sqrt_double_NEAREST(arg1[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsqrt64Fx4 (/*OUT*/V256* output,
                                           ULong a0, ULong a1, ULong a2,ULong a3) {

  double arg1[4] = {*((double*)(&a0)),*((double*)(&a1)), *((double*)(&a2)),*((double*)(&a3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sqrt_double_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONIC_softsqrt32F (Long a) {
  float *arg1 = (float*)(&a);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sqrt_float_SR_SMONOTONIC(*arg1, &res, backend_verrou_context);
}else{
  interflop_verrou_sqrt_float_NEAREST(*arg1, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsqrt32Fx8 (/*OUT*/V256* output,
					   ULong a0, ULong a1, ULong a2,ULong a3) {
  V256 reg1;   reg1.w64[0]=a0;   reg1.w64[1]=a1;   reg1.w64[2]=a2;   reg1.w64[3]=a3;
  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsqrt32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_SR_SMONOTONIC(arg1[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sqrt_float_NEAREST(arg1[i], res+i, backend_verrou_null_context);
  }
}
}


#endif
// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrouadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrouadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrouadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrouadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrousub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrousub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrousub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrousub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verroumul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroumul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroumul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroumul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verroudiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroudiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroudiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroudiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


#ifdef USE_VERROU_QUAD
// generation of operation add backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation sub backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation mul backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_mul_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_mul_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_mul_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_mul_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation div backend mcaquad


static VG_REGPARM(2) Long vr_mcaquaddiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_div_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_div_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_div_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquaddiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_div_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


#endif //USE_VERROU_QUAD
// generation of operation add backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_add_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_add_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}


// generation of operation sub backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}


// generation of operation mul backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_mul_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_mul_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_mul_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_mul_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_mul_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_mul_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}


// generation of operation div backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_div_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_div_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_div_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_div_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_div_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_div_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


#ifdef USE_VERROU_QUAD
// generation of operation add backend mcaquad


static VG_REGPARM(2) Long vr_mcaquad_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquad_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquad_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquad_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend mcaquad


static VG_REGPARM(2) Long vr_mcaquad_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquad_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquad_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquad_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend mcaquad


static VG_REGPARM(2) Long vr_mcaquad_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_mul_double(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_mul_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_mul_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquad_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquad_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_mul_float(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquad_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend mcaquad


static VG_REGPARM(2) Long vr_mcaquad_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_div_double(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_div_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_div_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquad_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquad_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_div_float(*arg1, *arg2, &res, backend_mcaquad_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquad_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquad_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


#endif //USE_VERROU_QUAD
// generation of operation add backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenorm_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_add_double(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenorm_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenorm_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_add_float(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenorm_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenorm_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenorm_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenorm_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenorm_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenorm_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_mul_double(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_mul_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_mul_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenorm_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenorm_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_mul_float(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_mul_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenorm_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_mul_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenorm_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_div_double(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_div_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkdenorm_div_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenorm_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenorm_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_div_float(*arg1, *arg2, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenorm_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_div_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenorm_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_div_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_maxadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_double(*arg1, *arg2, &res, backend_check_float_max_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_add_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_add_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_float(*arg1, *arg2, &res, backend_check_float_max_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_maxsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_double(*arg1, *arg2, &res, backend_check_float_max_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_float(*arg1, *arg2, &res, backend_check_float_max_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_maxmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_double(*arg1, *arg2, &res, backend_check_float_max_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_float(*arg1, *arg2, &res, backend_check_float_max_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_maxdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_double(*arg1, *arg2, &res, backend_check_float_max_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_div_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_div_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_float(*arg1, *arg2, &res, backend_check_float_max_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_max_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_double(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_add_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_add_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheck_float_max_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_float(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_add_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_max_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_double(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheck_float_max_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_float(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_sub_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_max_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_double(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheck_float_max_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_float(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_mul_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verroucheck_float_max_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_double(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_div_double(arg1[0], arg2[0], res, backend_check_float_max_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_check_float_max_div_double(arg1[1], arg2[1], res+1, backend_check_float_max_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheck_float_max_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_float(*arg1, *arg2, &res, backend_check_float_max_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheck_float_max_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_check_float_max_div_float(arg1[i], arg2[i], res+i, backend_check_float_max_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_NEARESTadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_NEARESTadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_NEARESTadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_NEARESTadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARDadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_UPWARDadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_UPWARDadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_UPWARDadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARDadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARDadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHESTadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_FARTHESTadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_FARTHESTadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_FARTHESTadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_ZEROadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_ZEROadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_ZEROadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_ZEROadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZEROadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZEROadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOMadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOMadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOMadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOMadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGEadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGEadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGEadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGEadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOMadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOMadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOMadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOMadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDETadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDETadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONICadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONICadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONICadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONICadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_NEARESTsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_NEARESTsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_NEARESTsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_NEARESTsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARDsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_UPWARDsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_UPWARDsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_UPWARDsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARDsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARDsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHESTsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_FARTHESTsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_FARTHESTsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_ZEROsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_ZEROsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_ZEROsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_ZEROsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZEROsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZEROsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOMsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOMsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOMsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOMsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGEsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGEsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGEsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOMsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOMsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOMsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDETsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDETsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONICsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONICsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONICsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONICsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_NEARESTmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_NEARESTmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_NEARESTmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_NEARESTmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARDmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_UPWARDmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_UPWARDmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_UPWARDmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARDmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARDmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHESTmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_FARTHESTmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_FARTHESTmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_FARTHESTmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_ZEROmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_ZEROmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_ZEROmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_ZEROmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZEROmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZEROmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOMmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOMmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOMmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOMmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGEmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGEmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGEmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGEmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOMmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOMmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOMmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOMmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDETmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDETmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONICmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONICmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONICmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONICmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_NEARESTdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_NEARESTdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_NEARESTdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_NEARESTdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_NEARESTdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARDdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_UPWARDdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_UPWARDdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARDdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_UPWARDdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARDdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARDdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_DOWNWARDdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHESTdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_FARTHESTdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_FARTHESTdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHESTdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_FARTHESTdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_ZEROdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_ZEROdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_ZEROdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZEROdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_ZEROdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZEROdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZEROdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZEROdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOMdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOMdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOMdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOMdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOMdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGEdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGEdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGEdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGEdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGEdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOMdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOMdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOMdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOMdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOMdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDETdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDETdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDETdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONICdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONICdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONICdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONICdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONICdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONICdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARD_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_UPWARD_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARD_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARD_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHEST_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_FARTHEST_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_ZERO_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_ZERO_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZERO_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZERO_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDET_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDET_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONIC_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONIC_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONIC_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONIC_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARD_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_UPWARD_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARD_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARD_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHEST_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_FARTHEST_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_ZERO_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_ZERO_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZERO_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZERO_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDET_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDET_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONIC_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONIC_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONIC_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONIC_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARD_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_UPWARD_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARD_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARD_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHEST_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_FARTHEST_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_ZERO_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_ZERO_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZERO_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZERO_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDET_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDET_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONIC_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONIC_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation mul backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONIC_softmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_mul_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_mul_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_mul_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_mul_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONIC_softmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_mul_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_mul_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_mul_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_UPWARD_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_UPWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_UPWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_UPWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_UPWARD_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_UPWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_UPWARD_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_UPWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_DOWNWARD_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_DOWNWARD(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_DOWNWARD(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_DOWNWARD(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_DOWNWARD_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_DOWNWARD(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_DOWNWARD_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_DOWNWARD(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_FARTHEST_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_FARTHEST(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_FARTHEST(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_FARTHEST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_FARTHEST_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_FARTHEST(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_FARTHEST_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_FARTHEST(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_ZERO_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_ZERO_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_ZERO_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_ZERO_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AWAY_ZERO_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_AWAY_ZERO(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AWAY_ZERO(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AWAY_ZERO(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AWAY_ZERO_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_AWAY_ZERO(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AWAY_ZERO_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AWAY_ZERO(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_RANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_DET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_DET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_RANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_DET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_COMDET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_COMDET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_RANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_COMDET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_AVERAGE(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_DET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_DET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_AVERAGE_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_DET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_COMDET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_COMDET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_AVERAGE_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_COMDET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_PRANDOM(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_DET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM_DET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM_DET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM_DET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_DET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_PRANDOM_DET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_DET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM_DET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_PRANDOM_COMDET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_PRANDOM_COMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_PRANDOM_COMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_PRANDOM_COMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_PRANDOM_COMDET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_PRANDOM_COMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_PRANDOM_COMDET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_PRANDOM_COMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_RANDOM_SCOMDET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_RANDOM_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_RANDOM_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_RANDOM_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_RANDOM_SCOMDET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_RANDOM_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_RANDOM_SCOMDET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_RANDOM_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_AVERAGE_SCOMDET_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_AVERAGE_SCOMDET(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_AVERAGE_SCOMDET(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_AVERAGE_SCOMDET(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_AVERAGE_SCOMDET_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_AVERAGE_SCOMDET(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_AVERAGE_SCOMDET_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_AVERAGE_SCOMDET(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_MONOTONIC_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_SR_MONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_SR_MONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_SR_MONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_MONOTONIC_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_SR_MONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_MONOTONIC_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_SR_MONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation div backend verrou


static VG_REGPARM(2) Long vr_verrou_SR_SMONOTONIC_softdiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_div_double_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softdiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_div_double_SR_SMONOTONIC(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double_SR_SMONOTONIC(arg1[1], arg2[1], res+1, backend_verrou_context);
}else{
  interflop_verrou_div_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_div_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softdiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_SR_SMONOTONIC(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_div_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verrou_SR_SMONOTONIC_softdiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_div_float_SR_SMONOTONIC(*arg1, *arg2, &res, backend_verrou_context);
}else{
  interflop_verrou_div_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softdiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verrou_SR_SMONOTONIC_softdiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_SR_SMONOTONIC(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_div_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verroucheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verroucheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


#ifdef USE_VERROU_QUAD
// generation of operation add backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


// generation of operation sub backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


#endif //USE_VERROU_QUAD
// generation of operation add backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_add_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_add_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


// generation of operation sub backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}


// generation of operation add backend verrou


static VG_REGPARM(2) Long vr_verroucheckcancellation_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheckcancellation_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend verrou


static VG_REGPARM(2) Long vr_verroucheckcancellation_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_verroucheckcancellation_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_verroucheckcancellation_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


#ifdef USE_VERROU_QUAD
// generation of operation add backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadcheckcancellation_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellation_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend mcaquad


static VG_REGPARM(2) Long vr_mcaquadcheckcancellation_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellation_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellation_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


#endif //USE_VERROU_QUAD
// generation of operation add backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormcheckcancellation_softadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_add_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_add_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_add_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_add_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellation_softadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_add_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_add_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_add_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_add_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_add_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation sub backend checkdenorm


static VG_REGPARM(2) Long vr_checkdenormcheckcancellation_softsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res+1, backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res+1, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_double_NEAREST(arg1[0], arg2[0], res, backend_verrou_null_context);
  interflop_verrou_sub_double_NEAREST(arg1[1], arg2[1], res+1, backend_verrou_null_context);
}
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
if(vr.instrument_soft){
  for(int i=0; i<4; i++){
     interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double_NEAREST(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellation_softsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res, backend_checkdenorm_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_sub_float_NEAREST(*arg1, *arg2, &res, backend_verrou_null_context);
}
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<8; i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellation_softsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;
if(vr.instrument_soft){
  for(int i=0; i<4;i++){
     interflop_checkdenorm_sub_float(arg1[i], arg2[i], res+i, backend_checkdenorm_context);
     interflop_checkcancellation_sub_float(arg1[i], arg2[i], res+i, backend_checkcancellation_context);
  }
}else{
  for(int i=0; i<4;i++){
     interflop_verrou_sub_float_NEAREST(arg1[i], arg2[i], res+i, backend_verrou_null_context);
  }
}
}


// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroumadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroumsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation madd backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation madd backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation madd backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquad_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_double(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquad_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_float(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquad_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_double(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquad_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_float(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation madd backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenorm_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenorm_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenorm_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenorm_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation madd backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation madd backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheckcancellation_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellation_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheckcancellation_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellation_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#ifdef USE_VERROU_QUAD
// generation of operation madd backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadcheckcancellation_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_double(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellation_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_float(*arg1, *arg2,  *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
//FMA Operator
static VG_REGPARM(3) Long vr_mcaquadcheckcancellation_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_double(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellation_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_mcaquad_madd_float(*arg1, *arg2, - *arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
#endif //USE_VERROU_QUAD
// generation of operation madd backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormcheckcancellation_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormcheckcancellation_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2,  *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
//FMA Operator
static VG_REGPARM(3) Long vr_checkdenormcheckcancellation_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormcheckcancellation_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_checkdenorm_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, - *arg3, &res, backend_checkcancellation_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheck_float_maxmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2,  *arg3, &res, backend_check_float_max_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheck_float_maxmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2,  *arg3, &res, backend_check_float_max_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheck_float_maxmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2, - *arg3, &res, backend_check_float_max_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheck_float_maxmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2, - *arg3, &res, backend_check_float_max_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheck_float_max_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2,  *arg3, &res, backend_check_float_max_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheck_float_max_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2,  *arg3, &res, backend_check_float_max_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verroucheck_float_max_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2, - *arg3, &res, backend_check_float_max_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheck_float_max_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2, - *arg3, &res, backend_check_float_max_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_NEARESTmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_NEARESTmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_UPWARDmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_UPWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_UPWARDmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_UPWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_DOWNWARDmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_DOWNWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_DOWNWARDmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_DOWNWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_FARTHESTmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_FARTHEST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_FARTHESTmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_FARTHEST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_ZEROmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_ZEROmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AWAY_ZEROmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AWAY_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AWAY_ZEROmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AWAY_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOMmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOMmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_DETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_DETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_COMDETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGEmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGEmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_DETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_DETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_COMDETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOMmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOMmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_DETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_DETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_COMDETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_SCOMDETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_SCOMDETmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDETmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_MONOTONICmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_SR_MONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_MONOTONICmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_SR_MONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_SMONOTONICmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_SR_SMONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONICmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_SR_SMONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_NEARESTmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_NEARESTmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_UPWARDmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_UPWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_UPWARDmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_UPWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_DOWNWARDmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_DOWNWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_DOWNWARDmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_DOWNWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_FARTHESTmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_FARTHEST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_FARTHESTmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_FARTHEST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_ZEROmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_ZEROmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AWAY_ZEROmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AWAY_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AWAY_ZEROmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AWAY_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOMmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOMmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_DETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_DETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_COMDETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGEmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGEmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_DETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_DETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_COMDETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOMmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOMmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_DETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_DETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_COMDETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_PRANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_PRANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_SCOMDETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_RANDOM_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_RANDOM_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_SCOMDETmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_AVERAGE_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDETmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_AVERAGE_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_MONOTONICmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_SR_MONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_MONOTONICmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_SR_MONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_SMONOTONICmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double_SR_SMONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONICmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float_SR_SMONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_UPWARD_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_UPWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_UPWARD_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_UPWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_DOWNWARD_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_DOWNWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_DOWNWARD_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_DOWNWARD(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_FARTHEST_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_FARTHEST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_FARTHEST_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_FARTHEST(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_ZERO_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_ZERO_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AWAY_ZERO_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AWAY_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AWAY_ZERO_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AWAY_ZERO(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_DET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_DET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_COMDET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_DET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_DET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_COMDET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_DET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_DET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM_DET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_COMDET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM_COMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_SCOMDET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_SCOMDET_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDET_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_SCOMDET(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_MONOTONIC_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_SR_MONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_MONOTONIC_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_SR_MONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation madd backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_SMONOTONIC_softmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_SR_SMONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONIC_softmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_SR_SMONOTONIC(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2,  *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_UPWARD_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_UPWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_UPWARD_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_UPWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_DOWNWARD_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_DOWNWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_DOWNWARD_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_DOWNWARD(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_FARTHEST_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_FARTHEST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_FARTHEST_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_FARTHEST(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_ZERO_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_ZERO_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AWAY_ZERO_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AWAY_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AWAY_ZERO_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AWAY_ZERO(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_DET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_DET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_COMDET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_COMDET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_DET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_DET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_COMDET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_COMDET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_DET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_DET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM_DET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_PRANDOM_COMDET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_PRANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_PRANDOM_COMDET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_PRANDOM_COMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_RANDOM_SCOMDET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_RANDOM_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_RANDOM_SCOMDET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_RANDOM_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_AVERAGE_SCOMDET_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_AVERAGE_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_AVERAGE_SCOMDET_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_AVERAGE_SCOMDET(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_MONOTONIC_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_SR_MONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_MONOTONIC_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_SR_MONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou
//FMA Operator
static VG_REGPARM(3) Long vr_verrou_SR_SMONOTONIC_softmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
if(vr.instrument_soft){
  interflop_verrou_madd_double_SR_SMONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_double_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verrou_SR_SMONOTONIC_softmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
if(vr.instrument_soft){
  interflop_verrou_madd_float_SR_SMONOTONIC(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
}else{
  interflop_verrou_madd_float_NEAREST(*arg1, *arg2, - *arg3, &res, backend_verrou_null_context);
}
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
